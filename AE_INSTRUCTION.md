# Instructions to reproduce the evaluations of the paper

Here are the detailed instructions to perform the same experiments in our paper.

## Artifact claims

We claim that the results might differ from those in our paper due to various factors (e.g., cluster sizes, machines, OS, software packages, etc.). Nevertheless, we expect DMTree to still achieve similar performance (in normal operations) with Cassandra while significantly reducing storage overhead (i.e., our main results). In addition, to reduce the influence of cloud storage location, hardware requirement, complexity, and running time of the evaluation, we made some changes to the evaluation configurations.

## Cluster setup

* We require **six compute nodes** and **one memory node** in AE.
* We provide a server cluster consisting of 7 nodes (skv-node1 to skv-node7) for reproducing the experiments in our paper. One node, `skv-node7`, will be made available as the jump host for accessing the cluster and as the main node for running the reproduction scripts. The AEC can submit their SSH public key via the following document: XXX. Once we have added the key to the server, node7 can be accessed using the following command:`ssh -P 6672 aefast26@222.195.68.87`.
* We sincerely apologize that, due to limited cluster resources, we are unable to provide each AEC with an independent execution environment. To prevent potential conflicts caused by concurrent usage, we have adopted a document-based reservation system along with an exclusive access notification mechanism in the following steps, ensuring that only one AEC can run experiments at any given time. 
* Additionally, to avoid excessive redundant testing, we will indicate which experiments have already been executed and have results available. It will be up to the AEC to decide whether to rerun those parts.

## Environment setup (~15 minutes)

We provide scripts to set up the environment for the evaluation, including cloning the code repository and copying and compiling it across multiple cluster nodes. The scripts are tested on Ubuntu 20.04 LTS. 

**Step 1:** Clone the code repository `aefast26`.

- Due to potential network fluctuations, the clone operation might require a few retries to complete successfully.

```shell
git clone https://github.com/muouim/aefast26.git
```

**Step 2:** Run the following script on `skv-node7`, including copying and compiling the code across multiple cluster nodes.

```shell
cd aefast26
bash build_ae.sh
```

To prevent repeated compilation and concurrent execution from disrupting the established environment, we have implemented a tracking and checking mechanism for the environment setup status when running the `build_ae.sh` script:

- If the script has not been executed before and the code has not been copied or compiled, the script will be executed.
- If the script is currently being executed by another AEC, a message will prompt: ”`The script is already running, please wait.`“
- If the script has already been executed, a message will prompt: "`Environment setup is complete, no need to run the script again.`"
- If AEC want to re-run the environment setup (some unexpected issues or something went wrong), please run `rm /tmp/build_ae.flag` the reset the environment setup status.

## Evaluations

This section describes how to reproduce the evaluations in our paper. To simplify the reproduction process, we provide Ansible-based scripts to run all major experiments. The script will automatically run the experiments and generate the result logs. The scripts will take about 9~10 days to finish all the experiments.

- **We suggest running the scripts of Exp#0 first, which can reproduce the main results of our paper while including most of the functionality verification (i.e., achieve controllable storage saving compared with Cassandra; provide similar performance of different types of KV operations such as read, write, scan, and update)**.

### Note on the experiment scripts

These evaluation scripts require a long time to run. To avoid the interruption of the experiments, we suggest running the scripts in the background with `tmux`, `nohup`, `screen`, etc. In addition, please make sure that all scripts have been given execution permissions. You can do this according to the following example:

```shell
cd scripts
find . -type f -name "*.sh" -exec chmod +x {} \;
```

### Note on the evaluation results

The raw running log generated by the YCSB benchmark tool will be stored in the folder `${PathToELECTResultSummary}/` configured in the `scripts/settings.sh` file. The log file will be named with detailed configuration information, such as workload, KV number, operation number, etc.

To make the result easier to read, we provide the summarized result of each evaluation in the `scripts/exp/` folder and named `${ExpName}.log`. For example, the result of Exp#1 will be stored in `scripts/exp/Exp1-ycsb.log`.

For the **performance evaluation**, the result will be summarized in different ways according to the running round number of the experiment (defined in each of the experiment scripts by the variable `RunningRoundNumber`).

* If the running round number is 1, the result will be directly output as in the example shown below.

```shell
[Exp info] scheme: elect, workload: Write, KVNumber: 600000, OPNumber: 60000, KeySize: 24, ValueSize: 1000, ClientNumber: 16, ConsistencyLevel: ONE, ExtraFlag: 
Throughput (unit: op/s): 
Only one round: 9992.38
[READ] Average operation latency (unit: us):
Only one round: 1369.53
[READ] 99th percentile latency (unit: us):
Only one round: 1883.00
```

For **other evaluations (i.e., Exp#3, 4, and 5)**, the result will be summarized similarly to the summarized performance results. Again, depending on the number of running rounds conducted, the output formats include options such as a single-round summary (run experiment with one round) or more comprehensive data sets featuring average, maximum, and minimum values (run experiment with 2~4 rounds), as well as average values with a 95% Student-T distribution confidence interval (run experiment more than five rounds). We provide examples of the summarized results of operation breakdown, recovery time cost, and resource usage in each related evaluation.

### Simple experiment (For quick verification)

#### Exp#0: Simple experiment (~ 10 hours)

We provide this simple experiment to verify our main experimental results quickly: **TODO: DMTree provides XXX...... similar performance compared to Cassandra while significantly reducing hot-tier storage overhead.** Specifically, we use 10M KV pairs and 1M KV operations (including read/write/update/scan, consistent with Exp2). This experiment will provide storage overhead (main results of Exp#1,2), performance of normal and degraded operations (main results of Exp#2), KV operation breakdown (main results of Exp#3), recovery time overhead when a single node fails (main results of Exp#4), and average resource utilization under load/normal/degraded conditions (main results of Exp#5). The summarized results will be printed on the screen after the evaluation and saved in the `scripts/exp/Exp0-simpleOverall.log` file.

You can run this simple experiment via the following command:

```shell
bash run_simple.sh
```

Alternatively, you may consider running it in the background to avoid keeping the terminal window open for an extended period.

```shell
nohup bash run_simple.sh >run_simple.output 2>&1 &
```

To prevent repeated experiments and concurrent execution from disrupting the running experiments, we have implemented a tracking and checking mechanism for the simple experiment status when running the `run_simple.sh` script:

- If the script has not been executed before and the code has not been copied or compiled, the script will be executed.
- If the script is currently being executed by another AEC, a message will prompt: ”`The script is already running, please wait.`“
- If the script has already been executed, a message will prompt: "`Simple experiment is complete, no need to run the script again.`".
- If AEC want to re-run the simple experiment (some unexpected results or something wrong), please run `rm /tmp/simple_exp.flag` the reset the simple experiment status.

The results will be output in the file `simple_results_uniform.csv` and `simple_results_zipfian.csv`. Here, we only show the title line and output sequence of each part. The specific result format of each part is as shown in the "Note on the evaluation results" above and the example of each specific experiment below.

```shell
Index,Workload,Total,Node1,Node2,Node4,Node5,Node6,Node7
dmtree,ycsb-c,53.32453,8.6159,9.12646,8.8498,8.79755,8.8378,9.09702
dmtree,insert-only,26.472190000000005,4.38358,4.7043,4.16385,4.21258,4.66381,4.34407
dmtree,update-only,29.65105,4.97043,4.96518,4.62563,4.88146,5.09017,5.11818
dmtree,scan-only,3.4183529999999998,0.56681,0.571155,0.568142,0.570995,0.570754,0.570497
fptree,ycsb-c,26.4043,4.40051,4.4354,4.38076,4.40077,4.38338,4.40348
fptree,insert-only,14.010290000000001,2.33336,2.35963,2.30048,2.33775,2.34299,2.33608
fptree,update-only,16.26618,2.71728,2.71469,2.65796,2.71978,2.72619,2.73028
fptree,scan-only,2.6468520000000004,0.440922,0.441329,0.440894,0.441491,0.441023,0.441193
sherman,ycsb-c,9.82585,1.63857,1.63852,1.63289,1.63897,1.63831,1.63859
sherman,insert-only,5.978708,0.998049,0.997185,0.994785,0.999086,0.994643,0.99496
sherman,update-only,1.883737,0.314246,0.312869,0.313135,0.313904,0.314071,0.315512
sherman,scan-only,2.9982810000000004,0.499906,0.499891,0.498906,0.499897,0.499846,0.499835
smart,ycsb-c,52.73223,7.37058,9.04472,7.81512,9.12709,9.69932,9.6754
smart,insert-only,11.291780000000001,1.72764,1.91582,1.55416,1.88719,2.13127,2.0757
smart,update-only,24.878660000000004,3.64825,4.31508,3.94635,4.24777,4.3604,4.36081
smart,scan-only,1.102827,0.184169,0.18495,0.182623,0.182861,0.184817,0.183407
rolex,ycsb-c,11.33694,1.8892,1.88971,1.88884,1.88942,1.89016,1.88961
rolex,insert-only,7.071890000000001,1.17945,1.17935,1.17467,1.17958,1.17948,1.17936
rolex,update-only,7.929880000000001,1.32325,1.32374,1.31062,1.32404,1.32429,1.32394
rolex,scan-only,3.3806950000000002,0.563801,0.563813,0.561656,0.563835,0.563788,0.563802
dlsm,ycsb-c,18.21639,3.0587,2.99944,2.91436,3.21047,2.9449,3.08852
dlsm,insert-only,5.38723,0.927232,0.884041,0.837977,0.788737,0.935613,1.01363
dlsm,update-only,42.774730000000005,3.7854,3.6829,9.31849,5.97053,9.91391,10.1035
dlsm,scan-only,0.779036,0.132139,0.132728,0.125136,0.127841,0.130415,0.130777
chime,ycsb-c,41.814569999999996,6.80939,7.01565,6.9761,6.94508,7.1876,6.88075
chime,insert-only,7.434680000000001,1.23387,1.24365,1.24164,1.2415,1.23394,1.24008
chime,update-only,18.70113,3.12817,3.10721,3.04182,3.12771,3.15147,3.14475
chime,scan-only,2.723014,0.449037,0.455628,0.453864,0.45466,0.453082,0.456743
```

The output experimental results correspond to those presented in Figures 11 and 12 of the original paper. To enable quick experimental verification, we only provide the bottleneck performance—**i.e., the performance of each baseline under each workload at the maximum thread count**—which corresponds to the **red-boxed data points in the figures** and has been converted into bar charts.

As shown in the original overall experiment figures, the red boxes highlight the data produced by the simple experiment, representing the performance of each baseline under each workload at the maximum thread count.![image-20250617173455219](.\AE_INSTRUCTION.assets\image-20250617173455219.png)

The experimental results in the file `simple_results.csv` are converted into bar charts, as shown in the figure.

<img src="D:\System\Adaption\AE\prepare\aefast26\AE_INSTRUCTION.assets\image-20250620115236555.png" alt="image-20250620115236555" style="zoom: 33%;" />

如果卡住了，请取消脚本，运行如下脚本，清除各个服务器上运行的程序，并从卡住的地方尝试重新运行

- 协程判定退出的时候，会存在一些卡住的情况？还是server掉了

```
nohup: ignoring input
---------- Phase 0: Check script running status ----------
Script started at: Tue 17 Jun 2025 05:18:53 PM UTC
This script has not been run before, starting the execution...
---------- Phase 1: Run simple experiment for each baseline ----------
=============================
Running experiment for DMTree
Finished DMTree
DMTree's results are output to /home/aefast26/aefast26/simple.output
Time after DMTree: Tue 17 Jun 2025 05:51:04 PM UTC
 - Elapsed since script started: 32.18 minutes
 - Time for this project: 32.18 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for FPTree
Finished FPTree
FPTree's results are output to /home/aefast26/aefast26/simple.output
Time after FPTree: Tue 17 Jun 2025 06:29:40 PM UTC
 - Elapsed since script started: 1.17 hours
 - Time for this project: 38.60 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for Sherman
Finished Sherman
Sherman's results are output to /home/aefast26/aefast26/simple.output
Time after Sherman: Tue 17 Jun 2025 07:36:40 PM UTC
 - Elapsed since script started: 2.29 hours
 - Time for this project: 1.11 hours
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for SMART
Finished SMART
SMART's results are output to /home/aefast26/aefast26/simple.output
Time after SMART: Tue 17 Jun 2025 08:26:05 PM UTC
 - Elapsed since script started: 3.12 hours
 - Time for this project: 49.36 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
Successfully parsed all workloads for baseline: smart
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for ROLEX
Finished ROLEX
ROLEX's results are output to /home/aefast26/aefast26/simple.output
Time after ROLEX: Tue 17 Jun 2025 09:17:36 PM UTC
 - Elapsed since script started: 3.97 hours
 - Time for this project: 51.46 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
Successfully parsed all workloads for baseline: smart
Successfully parsed all workloads for baseline: rolex
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for CHIME
Finished CHIME
CHIME's results are output to /home/aefast26/aefast26/simple.output
Time after CHIME: Tue 17 Jun 2025 10:03:38 PM UTC
 - Elapsed since script started: 4.74 hours
 - Time for this project: 45.98 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
Successfully parsed all workloads for baseline: smart
Successfully parsed all workloads for baseline: rolex
Successfully parsed all workloads for baseline: chime
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
=============================
Running experiment for dLSM
Finished dLSM
dLSM's results are output to /home/aefast26/aefast26/simple.output
Time after dLSM: Fri 20 Jun 2025 03:44:01 AM UTC
 - Elapsed since script started: 26.56 minutes
 - Time for this project: 26.56 minutes
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
Successfully parsed all workloads for baseline: smart
Successfully parsed all workloads for baseline: rolex
Successfully parsed all workloads for baseline: dlsm
Successfully parsed all workloads for baseline: chime
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
---------- Phase 2: Organize and output all the results ----------
========== Start Parsing ==========
Successfully parsed all workloads for baseline: dmtree
Successfully parsed all workloads for baseline: fptree
Successfully parsed all workloads for baseline: sherman
Successfully parsed all workloads for baseline: smart
Successfully parsed all workloads for baseline: rolex
Successfully parsed all workloads for baseline: dlsm
Successfully parsed all workloads for baseline: chime
======================================
Transaction throughput data has been saved to simple_results.csv
======================================
Script completed at: Fri 20 Jun 2025 03:44:08 AM UTC, Total elapsed time: 5.2 hours
---------- All phases completed. Output saved to /home/aefast26/aefast26/simple.output ----------
```

### Overall system analysis (Exp#11~14 in our paper)

#### Exp#11-#12: Performance with Micro-benchmarks (1 human minutes + ~ 20 compute-hours)

*Running:*

```shell
bash run_overall.sh
```

#### Exp#14: Performance with YCSB core workloads (1 human-minutes + ~ 5 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp2-operations.sh
```

#### Exp#13: Tail latency with Micro-benchmarks (1 human-minutes + ~ 5 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp3-breakdown.sh
```

*Results:* These summaries are available in the `scripts/exp/` directory and can be found in the file named `Exp3-breakdown.log`. For example, the write operation breakdown result of ELECT will be output as in the example shown below. Note that the title for each metric is the same as the title in the paper.

```shell
[Breakdown info for Write] scheme: elect, KVNumber: 6000000, KeySize: 24, ValueSize: 1000
WAL (unit: ms/MiB):
Only one round: 388.95
MemTable (unit: ms/MiB):
Only one round: 686.65
Flushing (unit: ms/MiB):
Only one round: 200.79
Compaction (unit: ms/MiB):
Only one round: 1638.06
Transitioning (unit: ms/MiB):
Only one round: 259.84
Migration (unit: ms/MiB):
Only one round: 17.72
...
```

### Parameter analysis (Exp#6~8 in our paper)

#### Exp#4: Full-node recovery (1 human-minutes + ~ 14 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp4-recovery.sh
```

*Results:* These summaries are available in the `scripts/exp/` directory and can be found in the file named `Exp4-recovery.log`.

* For ELECT, the recovery time is the time cost of retrieving the LSM-trees from the replication nodes and decoding the SSTables. The result will be output as in the example shown below.

```shell
[Exp info] scheme: elect, KVNumber: 6000000, KeySize: 24, ValueSize: 1000
Total recovery time cost (unit: s):
Average: 6653.00, Min: 6653, Max: 6653
Recovery time cost for retrieve LSM-trees (unit: s):
Average: 3442.00, Min: 3442, Max: 3442
Recovery time cost for decode SSTables (unit: s):
Average: 3211.00, Min: 3211, Max: 3211
```

* For Cassandra, the recovery time is the time cost of retrieving the SSTables from the replication only. The result will be output as in the example shown below.

```shell
[Exp info] scheme: cassandra, KVNumber: 6000000, KeySize: 24, ValueSize: 1000
Total recovery time cost (unit: s):
Only one round: 7515.00
```

#### Exp#5: Resource usage (1 human-minutes + ~ 5 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp5-resource.sh
```

*Results:* We summarize resource utilization as the 95th percentile of CPU usage, the 95th percentile of total memory overhead, total disk I/O, and total network overhead (bidirectional). In particular, we obtain the 95% percentile CPU usage based on the sum of the CPU usage of all nodes with the same timestamp and then calculate the average usage of each core (i.e., total usage/total number of cores). Therefore, the CPU usage results will be significantly affected by the differences in hardware configurations of different testbeds. The results will be output as in the example shown below.

```
[Resource usage with degraded operations] scheme: elect, KVNumber: 6000000, KeySize: 24, ValueSize: 1000, OPNumber: 600000
95%-percentile CPU Usage (%):
Only one round: 1.19
95%-percentile RAM Usage (GiB):
Only one round: 22.98
Total Disk I/O (GiB):
Only one round: 1.08
Total Network traffic (GiB):
Only one round: 202.38
```

#### Exp#6: Impact of key and value sizes (1 human-minutes + ~ 40 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp6-kvSize.sh
```

#### Exp#7: Impact of storage-saving target (1 human-minutes + ~ 45 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp7-balanceParam.sh
```

#### Exp#8: Impact of erasure coding parameters (1 human-minutes + ~ 12 compute-hours)

The original experiment requires at least 12 nodes (1 client node, 10 server nodes, and 1 storage node). For the provided testbeds, limited by the number of available nodes, we adapt the changing range of erasure code K from 4~8 to 2~4. This result only verifies ELECT's adaptability to different K values.

*Running:*

```shell
bash scripts/exp/Exp8-ecParam.sh
```

### System setting analysis (Exp#9,10 in our paper)

#### Exp#9: Impact of read consistency level (1 human-minutes + ~ 5 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp9-consistency.sh
```

#### Exp#10: Impact of number of clients (1 human minutes + ~ 5 compute-hours)

*Running:*

```shell
bash scripts/exp/Exp10-clients.sh
```

